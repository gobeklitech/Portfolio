{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regressor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSE 144 Winter 2022 Assignment 1\n",
        "\n",
        "In this project, the goal is to build a linear regression model to predict the fuel efficiency (MPG) of automobiles from the late 1970s to early 1980s. We'll be using the [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset from UCI Machine Learning Repository."
      ],
      "metadata": {
        "id": "R3SW8S-xw-ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import trange"
      ],
      "metadata": {
        "id": "O3qPeXtUw76-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pWICwO-d18N"
      },
      "outputs": [],
      "source": [
        "class DataModule:\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Inits data module with dataset url and column names. Do not modify.\n",
        "        \"\"\"\n",
        "        self.url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
        "        self.column_names = [\n",
        "            \"mpg\",\n",
        "            \"cylinders\",\n",
        "            \"displacement\",\n",
        "            \"horsepower\",\n",
        "            \"weight\",\n",
        "            \"acceleration\",\n",
        "            \"model_year\",\n",
        "            \"origin\",\n",
        "        ]\n",
        "\n",
        "    def load_data(self) -> None:\n",
        "        \"\"\"\n",
        "        Load the dataset and drop NaN rows. Do not modify.\n",
        "        \"\"\"\n",
        "        self.dataset = pd.read_csv(\n",
        "            self.url,\n",
        "            names=self.column_names,\n",
        "            na_values=\"?\",\n",
        "            comment=\"\\t\",\n",
        "            sep=\" \",\n",
        "            skipinitialspace=True,\n",
        "        ).dropna()\n",
        "\n",
        "    def bin_feature(self, feature: str, bins: pd.IntervalIndex) -> None:\n",
        "        \"\"\"\n",
        "        Perform binning operation on the column named \"feature\" in the given\n",
        "        dataframe, and encode the binned feature into one-hot vectors.\n",
        "\n",
        "        Args:\n",
        "            feature: Name of the feature to bin.\n",
        "            bins: Bin intervals represented by pd.IntervalIndex.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        self.dataset[feature] = pd.cut(self.dataset[feature] , bins)\n",
        "        self.dataset = pd.get_dummies(self.dataset , columns = [feature])\n",
        "\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def one_hot_encode(self, features: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Encode a list of features in a dataframe as one-hot vectors and drop the\n",
        "        (original) unencoded feature columns.\n",
        "\n",
        "        Args:\n",
        "            features: The column names of the features that need to be encoded.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        for feature in features:\n",
        "          bins = self.dataset[feature].unique()\n",
        "          bins = len(bins)\n",
        "          #self.dataset[feature] = pd.cut(self.dataset[feature],bins)\n",
        "          self.dataset = pd.get_dummies(self.dataset,columns = [feature])\n",
        "          #self.dataset[feature] = pd.get_dummies(self.dataset , columns = [feature])\n",
        "          #self.dataset.drop(column = feature)\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def cross_feature(self, feature_a: str, feature_b: str) -> None:\n",
        "        \"\"\"\n",
        "        Make a new crossed feature by multiplying feature_a and feature_b, and\n",
        "        name the new feature as \"crossed_feature\".\n",
        "\n",
        "        Args:\n",
        "            feature_a: The column name of feature A.\n",
        "            feature_b: The column name of feature B.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        self.dataset[\"crossed_feature\"] = self.dataset[feature_a]*self.dataset[feature_b]\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def normalize(self) -> None:\n",
        "        \"\"\"\n",
        "        Use min-max normalization to normalize the dataset. The equation is provided below.\n",
        "\n",
        "        d_normalized = (d - min(d)) / (max(d) - min(d))\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        #for column in self.dataset:\n",
        "          #max_val = self.dataset[column].max()\n",
        "          #min_val = self.dataset[column].min()\n",
        "          #self.dataset[column] = (self.dataset[column] - min_val) / (max_val - min_val)\n",
        "\n",
        "        max_val = self.dataset.max() \n",
        "        min_val = self.dataset.min()\n",
        "        self.dataset = (self.dataset - min_val) / (max_val - min_val)\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def train_val_test_split(\n",
        "        self, val_size: float = 0.2, test_size: float = 0.5, seed: int = 144\n",
        "    ) -> Tuple[Tuple[np.ndarray], Tuple[np.ndarray], Tuple[np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Split a dataframe into features and labels for train, validation, and test sets.\n",
        "\n",
        "        DO NOT modify the default parameters specified above, and make sure to turn on\n",
        "        shuffling by shuffle=True.\n",
        "\n",
        "        More specifically, you are supposed to first split 50% of all data into the test\n",
        "        set, and then the other 50% as the training set. For the training set, you should\n",
        "        then use the given 80%/20% split as train/validation.\n",
        "\n",
        "                               |--------|       |--|      |----------|\n",
        "                               train (156)    val (40)     test (196)\n",
        "\n",
        "        You should have 156/40/196 examples in train/val/test, respectively. And do not\n",
        "        forget to split features from labels at the end.\n",
        "\n",
        "        Args:\n",
        "            val_size: The proportion of the dataset to include in the validation set.\n",
        "            It must be a float type.\n",
        "\n",
        "            test_size: The proportion of the dataset to include in the validation set. It\n",
        "            must be a float type.\n",
        "\n",
        "            seed: Controls the shuffling of the dataframe. Do not modify the default seed.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing train, validation, and test features and labels.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        self.ydata = self.dataset['mpg'].copy()\n",
        "        self.xdata = self.dataset.drop(columns = 'mpg')\n",
        "\n",
        "        x_train , x_test , y_train , y_test = train_test_split(self.xdata , self.ydata , test_size = test_size , random_state = seed , shuffle = True)\n",
        "\n",
        "        x_train , x_val , y_train , y_val = train_test_split(x_train,y_train, test_size = val_size , random_state = seed , shuffle = True)\n",
        "        # print('\\n----- printing x_train -----\\,\\,')\n",
        "        # print(x_train)\n",
        "        # print('\\n----- printing x_test -----\\,\\,')\n",
        "        # print(x_test)\n",
        "        # print('\\n----- printing x_val -----\\,\\,')\n",
        "        # print(x_val)\n",
        "        # print('\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
        "        # print(len(x_train))\n",
        "        # print(len(x_test))\n",
        "        # print(len(x_val))\n",
        "        # print('\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "        return (\n",
        "            (x_train.to_numpy(), y_train.to_numpy()),\n",
        "            (x_val.to_numpy(), y_val.to_numpy()),\n",
        "            (x_test.to_numpy(), y_test.to_numpy()),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data module\n",
        "datamodule = DataModule()\n",
        "datamodule.load_data()\n",
        "feature_to_bin = \"model_year\"\n",
        "features_to_encode = [\"cylinders\", \"origin\"]\n",
        "features_to_cross = [\"displacement\", \"acceleration\"]\n",
        "intervals = pd.IntervalIndex.from_tuples([(69, 74), (74, 79), (79, 84)])\n",
        "\n",
        "# DO NOT MODIFY ANYTHING ABOVE\n",
        "\n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "datamodule.bin_feature(feature_to_bin,intervals)\n",
        "datamodule.one_hot_encode(features_to_encode)\n",
        "datamodule.cross_feature(features_to_cross[0],features_to_cross[1])\n",
        "datamodule.normalize()\n",
        "training_set , validation_set , test_set = datamodule.train_val_test_split()\n",
        "# ========== YOUR CODE ENDS HERE =========="
      ],
      "metadata": {
        "id": "gDCncF1vfoT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionTrainer:\n",
        "    def __init__(\n",
        "        self, num_features: int, learning_rate: float = 1e-3, num_epochs: int = 5000\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Inits the linear regression model.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.num_features = num_features\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "        self.test_loss = None\n",
        "\n",
        "        # Here the task is to initialize the parameters theta of the linear regression\n",
        "        # model. The initial parameters should be an NumPy array of zeros.\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        self.theta = np.zeros((num_features,))\n",
        "        # self.theta = np.random.normal(size=self.num_features + 1)\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    # def compute_cost(self,X: np.ndarray,y: np.ndarray):\n",
        "\n",
        "    #   self.data_size = y.size  # number of training examples\n",
        "      \n",
        "    #   J = 0\n",
        "    \n",
        "    #   for i in range(self.data_size):\n",
        "    #     x = X[i,:]\n",
        "    #     h = self.theta[0] + np.dot(self.theta[1:],x)\n",
        "    #     J += (h - y[i])**2\n",
        "\n",
        "    #   J = J*(1/(2*self.data_size))\n",
        "\n",
        "    #   return J \n",
        "\n",
        "\n",
        "    def gradient_descent_step(self, x: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Perform a single step of gradient update on self.theta.\n",
        "        \n",
        "        Args:\n",
        "            x: A matrix of features.\n",
        "            y: A vector of labels.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        self.data_size = y.size\n",
        "        self.theta = self.theta - (self.learning_rate)*self.mse_loss_derivative(x,y)\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        x_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        x_val: np.ndarray,\n",
        "        y_val: np.ndarray,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Run gradient descent for n epochs, where n = self.num_epochs. In every epoch,\n",
        "            1. Calculate the training loss given the current theta, and append it to\n",
        "               self.train_loss_history.\n",
        "            2. Calculate the validation loss given the current theta, and append it to\n",
        "               self.val_loss_history.\n",
        "            3. Update theta.\n",
        "\n",
        "        If you wish to use the bias trick, please remember to use it before the for loop.\n",
        "\n",
        "        Args:\n",
        "            x: A matrix of features.\n",
        "            y: A vector of labels.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "\n",
        "\n",
        "        for N in range(self.num_epochs):\n",
        "\n",
        "          self.train_loss_history.append(self.mse_loss(np.dot(x_train,self.theta),y_train))\n",
        "          self.val_loss_history.append(self.mse_loss(np.dot(x_val,self.theta),y_val))\n",
        "          self.gradient_descent_step(x_train,y_train)\n",
        "\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def mse_loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate the derivative of the loss function w.r.t. theta.\n",
        "\n",
        "        Args:\n",
        "            x: A matrix of features.\n",
        "            y: A vector of labels.\n",
        "\n",
        "        Returns:\n",
        "            A vector with the same dimension as theta, where each element is the\n",
        "            partial derivative of the loss function w.r.t. the corresponding element\n",
        "            in theta.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        #h = self.theta[0] + np.dot(self.theta[1:],x)\n",
        "\n",
        "        LD = np.dot(x,self.theta) - y\n",
        "\n",
        "        return np.dot(x.T,LD);\n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def evaluate(self, x_test: np.ndarray, y_test: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Evaluate the model on test set and store the test loss int self.test_loss. This\n",
        "        should be similar to step 1 and 2 in train().\n",
        "\n",
        "        If you used the bias trick in train(), you have to also use it here.\n",
        "\n",
        "        Args:\n",
        "            x_test: A matrix of features.\n",
        "            y_test: A vector of labels.\n",
        "        \"\"\"\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        \n",
        "        self.test_loss = self.mse_loss(np.dot(x_test,self.theta),y_test)\n",
        "     \n",
        "        # ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "    def mse_loss(self,pred: np.ndarray, target: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the mean squared error given prediction and target. The equation is\n",
        "        given below.\n",
        "\n",
        "        mse = sum((pred - target) ^ 2) / (2 * n)\n",
        "\n",
        "        Args:\n",
        "            pred: A vector of predictions.\n",
        "            target: A vector of labels.\n",
        "\n",
        "        Returns:\n",
        "            Mean squared error between each element in pred and target.\n",
        "        \"\"\"\n",
        "        assert pred.shape == target.shape\n",
        "        # ========== YOUR CODE STARTS HERE ==========\n",
        "        #mse = np.sum(np.square(pred - target)) / (2 * pred.size)\n",
        "        mse = np.sum(np.square(pred - target)) / (2 * pred.size)\n",
        "        return mse\n",
        "        # ========== YOUR CODE ENDS HERE =========="
      ],
      "metadata": {
        "id": "YZOF6w9lw3zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a linear regression trainer\n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "num_features = test_set[0].shape[1]\n",
        "trainer = LinearRegressionTrainer(num_features,num_epochs = 7000,learning_rate=.001)\n",
        "trainer.train(training_set[0],training_set[1],validation_set[0],validation_set[1])\n",
        "#print(trainer.train_loss_history)\n",
        "#print(trainer.val_loss_history)\n",
        "# ========== YOUR CODE ENDS HERE ==========\n",
        "\n",
        "# DO NOT MODIFY ANYTHING BELOW\n",
        "\n",
        "print(f\"Final train loss: {trainer.train_loss_history[-1]}\")\n",
        "print(f\"Final validation loss: {trainer.val_loss_history[-1]}\")\n",
        "print(trainer.theta)\n",
        "\n",
        "plt.plot(np.arange(trainer.num_epochs), trainer.train_loss_history, label=\"Train loss\")\n",
        "plt.plot(np.arange(trainer.num_epochs), trainer.val_loss_history, label=\"Val loss\")\n",
        "plt.title(\"Train + validation loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Use the below lines only after tuning hyperparameters\n",
        "# ========== YOUR CODE STARTS HERE ==========\n",
        "trainer.evaluate(test_set[0],test_set[1])\n",
        "# ========== YOUR CODE ENDS HERE ==========\n",
        "print(f\"Test loss: {trainer.test_loss}\")"
      ],
      "metadata": {
        "id": "1knVC-6KkTuV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "a3383890-4626-4b58-91ed-2ce23bf8fe03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final train loss: 0.003134024762898423\n",
            "Final validation loss: 0.0025989983388619705\n",
            "[ 0.45623676 -0.31455578 -0.44512625  0.2331801   0.10638585  0.17450682\n",
            "  0.31785502  0.0143469   0.16135998  0.16050856  0.10325332  0.15927894\n",
            "  0.15591906  0.21486643  0.22796221 -0.30256499]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0033495420258675317\n"
          ]
        }
      ]
    }
  ]
}